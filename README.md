
---

# ğŸ§  AI-Powered CV to Job Matcher

This project is an AI-based web scraping and job matching tool that leverages OCR, NLP, and LLMs to read a user's CV and suggest job opportunities that align with the candidateâ€™s skills and experience.

---

## ğŸš€ Features

- ğŸ“ **CV Parsing**: Converts PDF CVs into structured text using OCR and text block analysis.
- ğŸ§  **Semantic Matching**: Embeds CV content into a vector space for similarity-based job relevance matching using `Weaviate`.
- ğŸŒ **Web Scraping**: Scrapes job listings using `crawl4ai` and extracts structured data using LLM-based extraction.
- ğŸ“„ **Output**: Saves top matched jobs in a CSV for easy viewing and integration.

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ ocr.py                # Converts PDF to structured text
â”œâ”€â”€ main.py               # Embeds CV into Weaviate, runs scraper and LLM matching
â”œâ”€â”€  venue.py             # Contains scraping and job extraction logic
â”œâ”€â”€ models/
â”‚   â””â”€â”€ venue.py          # Job schema model
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_utils.py     # Helpers for saving data and filtering
â”‚   â””â”€â”€ scraper_utils.py  # Scraper-related utilities
â”œâ”€â”€ extracted_texts/      # Output folder for OCR-processed text
â”œâ”€â”€ extracted_images/     # Extracted images from the CV (if any)
â”œâ”€â”€ complete_venues.csv   # Final matched job listings
```

---

## âš™ï¸ Technologies Used & Why

### ğŸ”¹ Python ğŸ
Used for overall backend scripting due to its rich ecosystem in AI, OCR, and web scraping.

### ğŸ”¹ PyMuPDF (`fitz`) ğŸ“„
Used to extract both text and images from the uploaded PDF CV. It provides precise control over layout blocks, fonts, and text formatting.

### ğŸ”¹ Tesseract OCR (`pytesseract`) ğŸ”
Used to convert any images (scanned signatures, certificates, etc.) inside the CV to text. Helps enhance the completeness of the CV content before analysis.

### ğŸ”¹ LangChain ğŸ¦œğŸ”—
Used to split the CV into logical chunks and interface with vector databases like Weaviate. It simplifies working with large documents and embeddings.

### ğŸ”¹ HuggingFace Transformers ğŸ¤—
Provides high-quality language embeddings that represent the semantic meaning of the CV content. These are then stored in Weaviate.

### ğŸ”¹ Weaviate (Vector Database) ğŸ§¬ğŸ“Š
Used to store the semantic vector representation of the CV chunks and perform similarity search against job prompts. Weaviate is fast, scalable, and integrates smoothly with LangChain.

### ğŸ”¹ crawl4ai ğŸ¤–
An intelligent web scraping framework powered by LLMs. It allows automatic extraction of structured data from HTML pages using prompt-driven extraction logic.

### ğŸ”¹ AsyncIO â±ï¸ğŸ”„
Used to perform concurrent scraping across multiple pages without blocking, improving speed and performance.

### ğŸ”¹ LLM Extraction Strategy (`groq/deepseek`) ğŸ§ 
Used to extract only the most relevant jobs based on the userâ€™s CV context. It ensures meaningful matches instead of random job listings.

---

## ğŸ§ª How It Works

1. **Step 1 â€“ Add your CV**  
   Place your `cv.pdf` in the project's root directory.

2. . **Step 2 â€“ Run OCR & Job Matching**  
   Execute the main script to process your CV and match it against current job listings:
   ```bash
   python main.py
   ```

   This script performs the following:
   - Extracts text from your CV
   - Splits your CV into chunks
   - Embeds and uploads them to Weaviate (if not already processed)
   - Scrapes job listings
   - Matches listings using LLM-based extraction
   - Saves the final matched jobs

   - ğŸ“‚ Output: 
     - `extracted_texts/cv.txt`  â€“ Structured text extracted from your CV
     - `complete_venues.csv`  â€“ List of matched job opportunities

---

## ğŸ§© Internal Workflow

```
cv.pdf
  â†“ (ocr.py)
cv.txt
  â†“ (main.py)                     â†“ (scrape.py)
Vectorized Embeddings â†’ Weaviate â† Scraped Jobs â† Dialog.lk
         â†“                                â†“
     LLM-based Matching (LLMExtractionStrategy)
         â†“
     Output CSV: complete_venues.csv
```

---

## âœ… Requirements

### âš™ï¸ System Requirements (Host Machine)
- `Tesseract-OCR` â€“ for optical character recognition
- `Docker` & `Docker Compose` â€“ required if hosting `Weaviate` locally
- `Conda` (Miniconda or Anaconda) â€“ recommended for managing the Python environment


### ğŸ“¦ Project Environment Setup
1. Create and activate the Conda environment:
     ```bash
      conda create -n job-matcher python=3.9
      conda activate job-matcher
     ```

2. Install Python dependencies
     ```bash
     pip install -r requirements.txt
     ```

3. Install Chromium for Playwright
     ```bash
     playwright install chromium
     ```


### ğŸ“ Additional Setup
- Rename the `.env_sample` file to `.env` and configure the necessary environment variables.
  - To start Weaviate locally:
     ```bash
    cd docker
    docker-compose up -d
     ```
---

## ğŸ§  Future Improvements

- Upload CVs via a simple web interface
- Improve job source diversity (LinkedIn, Glassdoor, etc.)

---

## ğŸ‘¨â€ğŸ’» Author

Made with â¤ï¸ by Gihan Lakmal 

